# Обученная модель "Трансформер" для перевода с английского на русский язык

В данном репозитории представлена обученная модель ["Трансформер"](https://arxiv.org/abs/1706.03762) для выполнения нейронного машинного перевода с английского на русский язык и инструкция к её использованию. Для обучения была выбрана реализация модели из системы [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).

Содержание
=================
  * [Параметры модели](#параметры-модели)
  * [Обучающая выборка](#обучающая-выборка)
  * [BLEU](#bleu)
  * [Системные требования](#системные-требования)
  * [Установка](#установка)
  * [Перевод](#перевод)
  * [Благодарности](#благодарности)
  * [Список источников](#список-источников)

## Параметры модели
Реализация модели взята из системы [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py). Обучение производилось на протяжении 7 эпох со следующими параметрами:

```
layers: 6
rnn_size: 512 # (rnn_size - неудачное название параметра d_model в OpenNMT)
word_vec_size: 512
transformer_ff: 2048
heads: 8
dropout: 0.1
normalization: tokens
accum_count: 2
batch_size: 5000
batch_type: tokens
optim: adam
adam_beta2: 0.998
decay_method: noam
warmup_steps: 7300
learning_rate: 2
max_grad_norm: 0
param_init: 0
param_init_glorot: True
label_smoothing: 0.1
```

Полный список параметров и команда для запуска обучения приведены в каталоге `train`.

Размеры словарей:
  * src (en): 62490
  * tgt (ru): 62892

В репозитории имеется три варианта модели:
  * `models/transformer_epoch_7.pt` - модель, обученная на протяжении 7 эпох;
  * `models/transformer_epoch_7R.pt` - оптмизированный по требуемой памяти вариант первой модели, может быть использован только для перевода, но не для продолжения обучения;
  * `models/transformer_avg_last_10.pt` - модель с усреднёнными весами последних 10 чекпоинтов.
## Обучающая выборка
Обучающая выборка была составлена из нескольких наборов параллельных корпусов, представленных на сайте [OPUS](http://opus.nlpl.eu) (за исключением корпуса Yandex):
  * TED2013 — 100 тысяч предложений из субтитров к видеозаписям конференции TED 2013;
  * News-Commentary v11 — 200 тысяч предложений из новостных субтитров. Данная выборка ежегодно публикуется конференцией WMT для обучения моделей машинного перевода;
  * Wikipedia v1.0 — 600 тысяч предложений из статей Википедии;
  * EUbookshop v2 — 50 тысяч предложений из статей, изданных отделом публикаций Европейского Союза;
  * OpenSubtitiles v2018 — 27.4 миллиона предложений из субтитров к фильмам и сериалам с сайта OpenSubtitiles;
  * ParaCrawl v1 — 12.1 миллиона предложений из текстов, извлечённых веб-пауком проекта ParaCrawl;
  * [Yandex Corpus](https://translate.yandex.ru/corpus) v1.3 — 1 миллион предложений, случайным образом выбранных из экспериментальных корпусов, собранных компанией Яндекс в 2011-2013 годах из параллельных документов, найденных в Интернете в автоматическом режиме. Корпус предоставляется Яндексом бесплатно после прохождения регистрации.

Общий объём обучающей выборки после проведения фильтрации составил 6.17 миллионов предложений.

Выборка была токенизирована с помощью скрипта [tokenizer.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl) системы Moses и сегментирована с помощью инструмента [Subword-NMT](https://github.com/rsennrich/subword-nmt) (60000 операций слияния).

Выборку в двоичном формате можно загрузить по [ссылке](https://yadi.sk/d/yG0b2TnmQEVT5g).

## BLEU
  * оценка BLEU для модели `models/transformer_epoch_7.pt` составляет 31.7. 
  * оценка BLEU для модели `models/transformer_avg_last_10.pt` составляет 32.6; 
 
 Оценки были вычислены с помощью инструмента [SacreBLEU](https://github.com/mjpost/sacreBLEU) на детокенизированном тексте с флагом `-lc` (регистронезависимая оценка).

## Системные требования
  * четырёхядерный процессор;
  * 4 ГБ оперативной памяти;
  * операционная система семейства Linux.

## Установка

  1. Установить OpenNMT-py ([инструкция](http://opennmt.net/OpenNMT-py/main.html#installation))
  2. Клонировать данный репозиторий:
  ```
    git clone https://github.com/ManiaCello/en-ru-onmt
    cd en-ru-onmt
  ```
  3. Загрузить модель ([ссылка](https://yadi.sk/d/w6OL6sXqta38YA)) и поместить её в каталог `models`.
  4. * При переводе с помощью предлагаемого автором скрипта `translate.sh` открыть его с помощью блокнота и в разделе "параметры" задать параметры `WORK_DIR, ONMT_DIR, GPU, CPU_THREADS, BATCH`
      * При использовании API системы OpenNMT-py перейти в её репозиторий и запустить скрипт `setup.py`.
        ```
        cd OpenNMT-py
        python setup.py install
        ```

## Перевод
### Способы обращения к системе OpenNMT-py
Для выполнения перевода из командной строки может быть использован скрипт [translate.py](http://opennmt.net/OpenNMT-py/options/translate.html) системы OpenNMT-py. При необходимости встраивания функции перевода в другую программу можно использовать [API](http://opennmt.net/OpenNMT-py/onmt.translation.html) системы на языке Python. Пример его использования есть в вышеупомянутом скрипте. OpenNMT-py также имеет [REST-сервер](http://forum.opennmt.net/t/simple-opennmt-py-rest-server/1392), позволяющий сторонним программам обращаться к ней за переводом посредством WebAPI, но, к сожалению, его использование для данной модели затруднено отсутствием поддержки инструментов Moses и Subword-NMT (впрочем, при необходимости процесс модификации сервера вряд ли составит много труда).


### Формат входных и выходных данных модели
Модель работает с данными в следующем формате:
  * в одной строке содержится одно предложение для перевода;
  * предложения токенизированы с помощью скрипта tokenizer.perl;
  * предложения сегментированы с помощью скрипта apply_bpe.py (модель - `bpe_model.en.txt`).

Это означает, что исходные данные и перевод, полученный на выходе модели, требуют дополнительной обработки. Полный цикл перевода проиллюстрирован скриптом `translate.sh` и состоит из 3 этапов:
  1. токенизация и сегментация исходного предложения с помощью инструментов `tokenizer.perl` и `apply_bpe.py`;
  2. перевод с помощью системы OpenNMT-py;
  3. десегментация и детокенизация полученного перевода с помощью инструментов `sed` и `detokenizer.perl`

В случаях, когда необходимо перевести более одного предложения за раз, может понадобиться также разделение текста на предложения и представление их в виде "одно предложение на строку". В данной инструкции этот момент не рассматривается. Упомянем только, что он может быть реализован, например, при помощи библиотеки [NLTK](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt).

### Примеры

Пусть предложения, которые нужно перевести, содержатся в файле `data/gramma.en.txt`.

#### `translate.sh`
При переводе из командной строки можно использовать скрипт `translate.sh` (предварительно требуется задать параметры в разделе "параметры" текста скрипта). Пример его использования приведен в файле `example.sh`:
```
./translate.sh data/gramma.en.txt data/gramma.ru.txt
```
Первый параметр - путь к файлу с данными для перевода, второй - путь к файлу, в котрый нужно сохранить перевод.
 
Далее рассмотрим примеры основных этапов перевода, которые включает данный скрипт.
#### Обработка входных данных
Этап включает токенизацию и сегментацию данных (порядок важен!).

##### Токенизация
Токенизация - это разделение предложения на токены, которыми являются слова, знаки препинания и неалфавитные символы. Все токены отделяются друг от друга символом пробела.

Для токенизации используется скрипт `tools/tokenizer.perl`:
```
mkdir .tmp
tools/tokenizer.perl -l en -threads 8 < data/gramma.en.txt > .tmp/gramma.en.tok.txt
```
Параметр `threads` задаёт количество потоков, используемых при токенизации, и позволяет существенно ускорить этот процесс.

Пример предложения до токенизации:
```
She might snort up steam, snuff out fire!
```
После токенизации:
```
She might snort up steam , snuff out fire !
```
##### Сегментация
После токенизации выборка должна быть сегментирована методом [subword segmentation](https://www.aclweb.org/anthology/P16-1162). Сегментация применяется при обучении моделей нейронного машинного перевода для решения проблемы ограниченного словаря. Проблема заключается в том, что при фиксированном размере словаря, который требуют нейросетевые модели, слова не попавшие в него представляются одним токеном (`<unk>`). Метод subword segmentation предлагает разделение редко встречающихся слов на части, в расчёте на то, что уникальных частей намного меньше, чем уникальных слов, и части, попав в словарь, позволят модели перевести слово целиком. Метод оказывается действенным даже в тех случаях, когда слово вообще не встречалось в обучающей выборке, то есть даёт возможность делать "предположения" о переводе неизвестного слова.

Для сегментации используется скрипт `tools/apply_bpe.py`:
```
tools/apply_bpe.py -c tools/bpe_model.en.txt < .tmp/gramma.en.tok.txt > .tmp/gramma.en.seg.txt
```
`bpe_model.en.txt` - модель сегментации, полученная на использованной обучающей выборке.

Пример предложения до сегментации:
```
She might snort up steam , snuff out fire !
```
После сегментации:
```
She might sn@@ ort up steam , snuff out fire !
```
#### Перевод

Для перевода из командной строки система OpenNMT-py имеет скрипт [translate.py](http://opennmt.net/OpenNMT-py/options/translate.html).

Пример его использования:
```
cd OpenNMT-py
./translate.py -model /home/algernon/en-ru-onmt/models/transformer_avg.pt \
  -src .tmp/gramma.en.seg.txt \
  -output .tmp/gramma.ru.seg.txt \
  --gpu 0 \
  --batch_size 200 \
  --beam_size 4 \
  --alpha 0.6 \
  --length_penalty avg \
  --verbose &> /home/algernon/en-ru-onmt/translate.log
```

Параметры `beam_size, alpha, length_penalty` можно взять из скрипта `translate.sh`. Остальные требуют описания:
  * src - путь к файлу с входными данными;
  * output - путь к файлу, куда должен быть сохранён перевод;
  * gpu - номер процессора, на котором должен быть выполнен перевод. При переводе на CPU следует указать значение "-1". При использовании GPU номера имеющихся процессоров можно узнать с помощью команды `nvidia-smi`.
  * batch_size - размер пакета (число предложений, обрабатываемых за один шаг). Значение "200" подходит для GPU с 16 ГБ памяти. При появлении ошибок о нехватке памяти значение параметра следует уменьшить.

#### Обработка перевода

Перевод на выходе модели является токенизированным и сегментированным.

Для десегментации используется инструмент `sed`:
```
sed -r 's/(@@ )|(@@ ?$)//g' .tmp/gramma.ru.seg.txt > .tmp/gramma.ru.tok.txt
```

Для детокенизации используется скрипт `tools/detokenizer.perl`:
```
tools/detokenizer.perl -l ru < .tmp/gramma.ru.tok.txt > data/gramma.ru.txt
```

## Благодарности

Автор выражает благодарность компании Aligned Research Group за предоставленные вычислительные мощности для обучения нейронной сети.

## Список источников

[Vaswani et al., "Attention is all you need"](https://arxiv.org/abs/1706.03762)

[OPUS - an open source parallel corpus](http://opus.nlpl.eu/)

[Яндекс.Переводчик. Англо-русский параллельный корпус](https://translate.yandex.ru/corpus)

[OpenNMT: Neural Machine Translation Toolkit](https://arxiv.org/pdf/1805.11462)

[OpenNMT technical report](https://doi.org/10.18653/v1/P17-4012)

[OpenNMT-py - Open Source Neural Machine Translation in PyTorch](https://github.com/OpenNMT/OpenNMT-py)

[Unsupervised Word Segmentation for Neural Machine Translation and Text Generation](https://github.com/rsennrich/subword-nmt)

[Cynical Selection - Allo-media data selection tool ](https://github.com/allo-media/cynical-selection)

[SacreBLEU - Reference BLEU implementation that auto-downloads test sets and reports a version string to facilitate cross-lab comparisons](https://github.com/mjpost/sacreBLEU)